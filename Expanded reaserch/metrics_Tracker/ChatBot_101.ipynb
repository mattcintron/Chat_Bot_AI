{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx3FSoX79q50"
   },
   "source": [
    "## Types of Chat Bot's\n",
    "In the world of machine learning and AI there are many different kinds of chat bots. Some chat bots are virtual assistants, others are just there to talk to, some are customer support agents and you've probably seen some of the ones used by businesses to answer questions. For this tutorial we will be creating a relatively simple chat bot that will be be used to answer frequently asked questions.\n",
    "\n",
    "## Install Packages\n",
    "Before starting to work on our chatbot we need to download a few python packages. Please note as of writing this these packages will ONLY WORK IN PYTHON 3.6. Hopefully this will be fixed in the future.\n",
    "\n",
    "We will simply use pip to install the following:\n",
    "- numpy\n",
    "- nltk\n",
    "- tensorflow\n",
    "- tflearn\n",
    "\n",
    "Simply go to CMD and type: pip install \"package name\". Where you will replace \"package_name\" with all of the entries listed above.\n",
    "\n",
    "### Training Data\n",
    "Now it's time to understand what kind of data we will need to provide our chatbot with. Since this is a simple chatbot we don't need to download any massive datasets. We will just use data that we write ourselves. To follow along with the tutorial properly you will need to create a .JSON file that contains the same format as the one seen below. I've called my file \"intents.json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "arqTfK5C92tB",
    "outputId": "5e9fe991-8d45-4a20-f680-3fbf3bd4aab2"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6w8PcYqN-KVB",
    "outputId": "776d27a5-af24-47a6-913e-c855e56e684c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /content/drive/MyDrive/Colab Notebooks/notebooks: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls '/content/drive/MyDrive/Colab Notebooks/notebooks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTwyJtSM9q54"
   },
   "source": [
    "What we are doing with the JSON file is creating a bunch of messages that the user is likely to type in and mapping them to a group of appropriate responses. The tag on each dictionary in the file indicates the group that each message belongs too. With this data we will train a neural network to take a sentence of words and classify it as one of the tags in our file. Then we can simply take a response from those groups and display that to the user. The more tags, responses, and patterns you provide to the chatbot the better and more complex it will be.\n",
    "\n",
    "### Loading our JSON Data\n",
    "We will start by importing some modules and loading in our json data. Make sure that your .json file is in the same directory as your python script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nIlzU6O9q55",
    "outputId": "b54f40ee-674e-4e1e-83bb-c4c047dd4f6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dacarpen/.pyenv/versions/3.6.14/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import json\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5IF9UjK-xJl",
    "outputId": "f358bfb1-84da-41e6-8b55-8d6eaa766d4e"
   },
   "outputs": [],
   "source": [
    "#!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-SDEz2__Pug",
    "outputId": "02c454a8-7d38-4f48-ab0f-e67fb2730b30"
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNZkFuRf9q57"
   },
   "source": [
    "## Extracting Data\n",
    "Now its time to take out the data we want from our JSON file. We need all of the patterns and which class/tag they belong to. We also want a list of all of the unique words in our patterns (we will talk about why later), so lets setup some blank lists to store these values.\n",
    "\n",
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "Now its time to loop through our JSON data and extract the data we want. For each pattern we will turn it into a list of words using nltk.word_tokenizer, rather than having them as strings. We will then add each pattern into our docs_x list and its associated tag into the docs_y list.\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "        \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "In the next tutorial we will do some preprocessing of this data and get it ready to feed to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhQaBbIb9q58",
    "outputId": "5a477d5a-272c-42ea-84a8-3ddb5ce64474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['greeting', 'goodbye', 'what', 'USCIS', 'DACA', 'DACA_DETAIL', 'hours']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "        \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "        \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJtmQ-Zv9q58"
   },
   "source": [
    "## Word Stemming\n",
    "You may have heard me talk about word stemming in the previous tutorial. Stemming a word is attempting to find the root of the word. For example, the word \"thats\" stem might be \"that\" and the word \"happening\" would have the stem of \"happen\". We will use this process of stemming words to reduce the vocabulary of our model and attempt to find the more general meaning behind sentences.\n",
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "labels = sorted(labels)\n",
    "\n",
    "This code will simply create a unique list of stemmed words to use in the next step of our data preprocessing.\n",
    "\n",
    "## Bag of Words\n",
    "Now that we have loaded in our data and created a stemmed vocabulary it's time to talk about a bag of words. As we know neural networks and machine learning algorithms require numerical input. So out list of strings wont cut it. We need some way to represent our sentences with numbers and this is where a bag of words comes in. What we are going to do is represent each sentence with a list the length of the amount of words in our models vocabulary. Each position in the list will represent a word from our vocabulary. If the position in the list is a 1 then that will mean that the word exists in our sentence, if it is a 0 then the word is nor present. We call this a bag of words because the order in which the words appear in the sentence is lost, we only know the presence of words in our models vocabulary.\n",
    "\n",
    "As well as formatting our input we need to format our output to make sense to the neural network. Similarly to a bag of words we will create output lists which are the length of the amount of labels/tags we have in our dataset. Each position in the list will represent one distinct label/tag, a 1 in any of those positions will show which label/tag is represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tA54sF3e9q59",
    "outputId": "75b713d8-4ef4-40aa-d9e5-ba2cfdd49331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "labels = sorted(labels)\n",
    "\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "\n",
    "training = numpy.array(training)\n",
    "output = numpy.array(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zixy4xyX9q5-"
   },
   "source": [
    "## Developing a Model\n",
    "Now that we have preprocessed all of our data we are ready to start creating and training a model. For our purposes we will use a fairly standard feed-forward neural network with two hidden layers. The goal of our network will be to look at a bag of words and give a class that they belong too (one of our tags from the JSON file).\n",
    "\n",
    "We will start by defining the architecture of our model. Keep in mind that you can mess with some of the numbers here and try to make an even better model! A lot of machine learning is trial an error.\n",
    "\n",
    "\n",
    "## Training & Saving the Model\n",
    "Now that we have setup our model its time to train it on our data! To do these we will fit our data to the model. The number of epochs we set is the amount of times that the model will see the same information while training.\n",
    "\n",
    "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save(\"model.tflearn\")\n",
    "Once we are done training the model we can save it to the file model.tflearn for use in other scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3AWDLVoX9q5_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dacarpen/.pyenv/versions/3.6.14/lib/python3.6/site-packages/tflearn/initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "ops.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeoAtP8NCEN1",
    "outputId": "eaf6de6d-f189-4085-e6b6-30a16e0969f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.01236\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1000 | loss: 0.01236 - acc: 1.0000 -- iter: 24/30\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.01252\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1000 | loss: 0.01252 - acc: 1.0000 -- iter: 30/30\n",
      "--\n",
      "INFO:tensorflow:/Users/dacarpen/git/CI5-Chat-in-the-Hat/statTracker/model_chat101/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save(\"model_chat101/model.tflearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24GpdX8k9q6A"
   },
   "source": [
    "## Making Predictions\n",
    "Now its time to actually use the model! Ideally we want to generate a response to any sentence the user types in. To do this we need to remember that our model does not take string input, it takes a bag of words. We also need to realize that our model does not spit out sentences, it generates a list of probabilities for all of our classes. This makes the process to generate a response look like the following:\n",
    "– Get some input from the user\n",
    "– Convert it to a bag of words\n",
    "– Get a prediction from the model\n",
    "– Find the most probable class\n",
    "– Pick a response from that class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlKDv0zY9q6A"
   },
   "source": [
    "The bag_of_words function will transform our string input to a bag of words using our created words list. The chat function will handle getting a prediction from the model and grabbing an appropriate response from our JSON file of responses.\n",
    "\n",
    "Now run the program and enjoy chatting with your bot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url=\"http://localhost:5000/record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "id": "9U1vM_is9q6A",
    "outputId": "56e27e22-171b-4d80-900c-6b9c291f7f42"
   },
   "outputs": [],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)\n",
    "\n",
    "def chat():\n",
    "    print(counts)\n",
    "    print(\"Start talking with the bot (type quit to stop)!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words)])\n",
    "        results_index = numpy.argmax(results)\n",
    "        results_score = numpy.amax(results)\n",
    "        tag = labels[results_index]\n",
    "        #record the result for analysis\n",
    "        recordData = {'intent':tag, 'count':1, 'score': \"{:.4f}\".format(results_score), 'input': inp}\n",
    "        x = requests.post(url, json = recordData )\n",
    "        # print(x.text)   \n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "\n",
    "        print(random.choice(responses))\n",
    "        \n",
    "def question(text):\n",
    "    results = model.predict([bag_of_words(text, words)])\n",
    "    results_index = numpy.argmax(results)\n",
    "    results_score = numpy.amax(results)\n",
    "    tag = labels[results_index]\n",
    "    recordData = {'intent':tag, 'count':1, 'score': \"{:.4f}\".format(results_score), 'input': text}\n",
    "    x = requests.post(url, json = recordData )\n",
    "    for tg in data[\"intents\"]:\n",
    "        if tg['tag'] == tag:\n",
    "            responses = tg['responses']\n",
    "\n",
    "    result = (random.choice(responses))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# run this to chat \n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this to send individual data \n",
    "question('hello?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up'], 'responses': ['Hello!', 'Good to see you again!', 'Hi there, how can I help?'], 'context_set': ''}\n",
      "{'tag': 'goodbye', 'patterns': ['cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day'], 'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'], 'context_set': ''}\n",
      "{'tag': 'what', 'patterns': ['what are you', 'who are you', 'what do you do', 'what is your purpose', 'what is this'], 'responses': ['I am USCIS chat bot for handeling questions and customer service', 'I am a full AI chat bot for USCIS'], 'context_set': ''}\n",
      "{'tag': 'USCIS', 'patterns': ['what is USCIS', 'USCIS dose what?', 'whats USCIS do?'], 'responses': [\"U.S. Citizenship and Immigration Services is an agency of the United States Department of Homeland Security that administers the country's naturalization and immigration system\"], 'context_set': ''}\n",
      "{'tag': 'DACA', 'patterns': ['What is purpose of DACA?', 'What is deferred action?', 'What is DACA?'], 'responses': ['Under existing regulations, an individual whose case has been deferred is eligible to receive employment authorization for the period of deferred action, provided he or she can demonstrate an economic necessity for employment. DHS', 'The purpose of DACA is to protect eligible immigrant youth who came to the United States when they were children from deportation.'], 'context_set': ''}\n",
      "{'tag': 'DACA_DETAIL', 'patterns': ['Why is DACA ending?', 'Why is DACA phasing out', 'How can I get more DACA information?', 'Frequently asked DACA questions'], 'responses': ['Taking into consideration the federal court rulings in ongoing litigation, and the September 4, 2017 letter from the Attorney General, it is clear that program should be terminated. As such, the Acting Secretary of Homeland Security rescinded the June 15, 2012 memorandum establishing the DACA program. Please see the Attorney General’s letter and the Acting Secretary of Homeland Security’s memorandum for further information on how this decision was reached.', 'You can get More information in the URL (https://www.dhs.gov/news/2017/09/05/frequently-asked-questions-rescission-deferred-action-childhood-arrivals-daca)!'], 'context_set': ''}\n",
      "{'tag': 'hours', 'patterns': ['when are you open', 'what are your hours', 'hours of operation', 'what is your availability'], 'responses': ['We are open 7am-4pm Monday-Friday!'], 'context_set': ''}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('intents.json',)\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "for i in data['intents']:\n",
    "    print(i)\n",
    "  \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up'], ['cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day'], ['what are you', 'who are you', 'what do you do', 'what is your purpose', 'what is this'], ['what is USCIS', 'USCIS dose what?', 'whats USCIS do?'], ['What is purpose of DACA?', 'What is deferred action?', 'What is DACA?'], ['Why is DACA ending?', 'Why is DACA phasing out', 'How can I get more DACA information?', 'Frequently asked DACA questions'], ['when are you open', 'what are your hours', 'hours of operation', 'what is your availability']]\n",
      "\n",
      "\n",
      "['Hello!', 'Sad to see you go :(', 'I am USCIS chat bot for handeling questions and customer service', \"U.S. Citizenship and Immigration Services is an agency of the United States Department of Homeland Security that administers the country's naturalization and immigration system\", 'Under existing regulations, an individual whose case has been deferred is eligible to receive employment authorization for the period of deferred action, provided he or she can demonstrate an economic necessity for employment. DHS', 'Taking into consideration the federal court rulings in ongoing litigation, and the September 4, 2017 letter from the Attorney General, it is clear that program should be terminated. As such, the Acting Secretary of Homeland Security rescinded the June 15, 2012 memorandum establishing the DACA program. Please see the Attorney General’s letter and the Acting Secretary of Homeland Security’s memorandum for further information on how this decision was reached.', 'We are open 7am-4pm Monday-Friday!']\n"
     ]
    }
   ],
   "source": [
    "questions =[]\n",
    "answers = []\n",
    "\n",
    "for i in data['intents']:\n",
    "    questions.append(i['patterns'])\n",
    "    answers.append(i['responses'][0])\n",
    "    \n",
    "print(questions)\n",
    "print('')\n",
    "print('')\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parrot loaded ready to work\n"
     ]
    }
   ],
   "source": [
    "# set up parrot to do parra phrases \n",
    "# Import libraries\n",
    "from parrot import Parrot\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('parrot loaded ready to work')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Init models (make sure you init ONLY once if you integrate this to your code)\n",
    "parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['list the most delicious papayas you have ever eaten?', 'tell me the most delicious papaya you have ever had?', 'tell me the best and sexiest papaya dish?', \"what's the most delicious papaya you've ever had?\", 'show some of the best papayas?', 'which is the most delicious papaya?', \"what's the most delicious papaya?\", 'what is the most delicious papaya?']\n"
     ]
    }
   ],
   "source": [
    "#build compresed method \n",
    "def get_para(phrase, num, length):\n",
    "    texts = []\n",
    "    para_phrases = parrot.augment(input_phrase=phrase, use_gpu=False, diversity_ranker='levenshtein', do_diverse=False,\n",
    "                                max_return_phrases=num, max_length=length, adequacy_threshold=0.9, fluency_threshold=0.9)\n",
    "    for para_phrase in para_phrases:\n",
    "        text = para_phrase[0]\n",
    "        texts.append(text)\n",
    "    \n",
    "    return texts\n",
    "        \n",
    "\n",
    "text = 'Whats the most delicious papayas?'\n",
    "texts = get_para(text, 10, len(text)+5)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Hi']\n",
      "[\"what's your general state?\", \"what's up?\", 'how are you doing?', 'how are you?', 'How are you']\n",
      "['who is there?', 'is there anybody there?', 'is anyone here?', 'is anyone there?', 'Is anyone there?']\n",
      "['Hello', 'Hello']\n",
      "['happy day', 'good day', 'Good day']\n",
      "[\"what's going on?\", 'what happened?', 'what was wrong?', \"what's up?\", \"what's up\", 'Whats up']\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "['cya', 'cya']\n",
      "['see you later', \"we'll see you later\", \"maybe i'll see you later\", 'See you later']\n",
      "['sorry', 'goodbye', 'Goodbye']\n",
      "[\"i'm going to leave\", \"i'm leaving\", 'I am Leaving']\n",
      "['have a good day', 'Have a Good day']\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "['what are you doing?', 'what are you like?', 'what are you?', 'what are you']\n",
      "['who are you?', 'who are you']\n",
      "[\"what's the thing you're doing?\", 'what can you do?', 'what do you do?', 'what do you do']\n",
      "[\"what's your goal?\", \"what's your purpose?\", 'what is your purpose?', 'what is your purpose']\n",
      "[\"what's going on?\", 'what?', 'what is it?', \"what's this?\", 'what is this?', 'what is this']\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0]\n",
      "[\"what's uscis?\", 'what is uscis?', 'what is USCIS']\n",
      "['how can the uscis handle this?', 'what is uscis planning?', 'uscis has to pay what?', 'what do the uscis do?', 'USCIS dose what?']\n",
      "['what does uscis does?', 'what do the uscis do?', 'what does uscis do?', 'what does the uscis do?', 'whats USCIS do?']\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['what are daca benefits and how does the program work?', 'how do you use daca for legal purposes?', 'how do you understand daca?', 'tell me the purpose of daca?', \"what's the purpose of daca?\", 'What is purpose of DACA?']\n",
      "['what is a delayed action? what is the difference between this and real action?', 'tell me the concept of deferred action?', 'what is delayed action?', 'what are deferred actions?', 'what is a deferred action?', 'what is deferred action?', 'What is deferred action?']\n",
      "['tell me the daca?', 'what exactly is daca?', 'what is daca?', 'What is DACA?']\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n",
      "['why is the daca now over?', 'why did daca go away?', 'why has daca ended?', 'do you see how daca ends?', 'why is the daca ending?', 'Why is DACA ending?']\n",
      "['why do some people want daca gone?', 'why was daca scrapped?', 'what causes daca to repeal?', 'why is the daca program phasing out?', 'why is daca out?', 'why is daca phasing out?', 'Why is DACA phasing out']\n",
      "['how do i get information about daca?', 'how do i get more information about daca?', 'how can i get information about daca?', 'where can i get more details about the daca?', 'where can i get more information about daca?', 'how can i get more information about daca?', 'how do i get more daca information?', 'how can i get more daca information?', 'How can I get more DACA information?']\n",
      "['do you have questions about daca?', 'Frequently asked DACA questions']\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0]\n",
      "[\"when's the door open?\", 'when do you open?', 'when are you up?', 'when are you open?', 'when are you open']\n",
      "[\"what's your work hours?\", 'show me your hours?', 'how are your hours?', 'what are your hours?', 'what are your hours']\n",
      "['hours of operation', 'hours of operation']\n",
      "['how is your availability?', 'what is your availability?', 'what is your availability']\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]\n",
      "\n",
      "these are the final results\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# now run all intents and make 0 succsess 1 fail\n",
    "results = []\n",
    "\n",
    "index =0\n",
    "for x in questions:\n",
    "    answer = answers[index]\n",
    "    for i in x:\n",
    "        texts = get_para(i, 10, len(text)+5)\n",
    "        texts.append(i)\n",
    "        print(texts)\n",
    "        for text in texts:\n",
    "            res = question(text)\n",
    "            #print('AI :', res)\n",
    "            #print('answer :', answer)\n",
    "            if(answer == res):\n",
    "                results.append(0)\n",
    "            else:\n",
    "                results.append(1)\n",
    "    index +=1\n",
    "    print(results)\n",
    "\n",
    "print('')\n",
    "print('these are the final results')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how many streets must a man walk before he is considered to be a man?', 'how many streets must a man walk before he can be considered a man?', \"How many roads must a man walk down before he's considered a man\"]\n",
      "['where can i find the form 158 b?', 'where can i find the form 158-b?', 'where can i find a form 158-b?', 'Where can I find Form-158-b']\n",
      "['do i need to include a location for remote work sites?', 'will i need to include a site for remote work sites?', 'do i need to include a hiring site for remote work locations?', 'Do I need to include a hiring site for remote work locations']\n",
      "[\"what's naics code?\", \"what's an naics code?\", 'what is naics code?', 'what is a naics code?', 'What is a NAICS code?']\n",
      "['is my tax exemption applied to my case?', 'are there exceptions?', 'am i exempt?', 'are i exempt?', 'am I exempt?']\n",
      "['can I start the employee working before E-verify returns a final asnwer', 'can I start the employee working before E-verify returns a final asnwer']\n"
     ]
    }
   ],
   "source": [
    "# now run new questions\n",
    "newQuestions = [\"How many roads must a man walk down before he's considered a man\", \"Where can I find Form-158-b\", \"Do I need to include a hiring site for remote work locations\",\n",
    "               \"What is a NAICS code?\", \"am I exempt?\", \"can I start the employee working before E-verify returns a final asnwer\"]\n",
    "\n",
    "for i in newQuestions:\n",
    "    texts = get_para(i, 10, len(text)+5)\n",
    "    texts.append(i)\n",
    "    print(texts)\n",
    "    for text in texts:\n",
    "        res = question(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ChatBot_101.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
